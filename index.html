<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <script src="https://cdn.jsdelivr.net/npm/protobufjs@7.X.X/dist/protobuf.min.js"></script>
    <title>Pipecat WebSocket Client Example</title>
    <style>
        /* ... previous styles remain the same ... */
    </style>
</head>
<body>
    <h1>Pipecat WebSocket Client Example</h1>
    <h3>
        <div id="progressText">Loading, please wait...</div>
        <div class="status-indicator" id="connectionStatus"></div>
    </h3>
    <div class="controls">
        <button id="startAudioBtn">Start Call</button>
        <button id="stopAudioBtn">End Call</button>
    </div>
    <div class="audio-stats" id="audioStats"></div>

    <script>
        // Constants
        const SAMPLE_RATE = 24000;
        const NUM_CHANNELS = 1;
        const BUFFER_SIZE = 2048;
        const MAX_BUFFER_SIZE = 10;
        const PLAY_TIME_RESET_THRESHOLD_MS = 1.0;

        // State variables
        let Frame = null;
        let ws = null;
        let audioContext = null;
        let source = null;
        let microphoneStream = null;
        let scriptProcessor = null;
        let playTime = 0;
        let lastMessageTime = 0;
        let isPlaying = false;
        let audioQueue = [];
        let statsInterval = null;
        let workletNode = null;

        // DOM elements
        const startBtn = document.getElementById('startAudioBtn');
        const stopBtn = document.getElementById('stopAudioBtn');
        const progressText = document.getElementById('progressText');
        const connectionStatus = document.getElementById('connectionStatus');
        const audioStats = document.getElementById('audioStats');

        // Initialize Protobuf
        protobuf.load('frames.proto', (err, root) => {
            if (err) {
                progressText.textContent = 'Error loading protocol buffer definition';
                console.error(err);
                return;
            }
            Frame = root.lookupType('pipecat.Frame');
            progressText.textContent = 'Ready to start. Click "Start Call" to begin.';
            startBtn.disabled = false;
        });

        // Initialize audio context only once
        async function initializeAudio() {
            if (!audioContext) {
                audioContext = new (window.AudioContext || window.webkitAudioContext)({
                    latencyHint: 'interactive',
                    sampleRate: SAMPLE_RATE
                });

                try {
                    await audioContext.audioWorklet.addModule('audioProcessor.js');
                } catch (err) {
                    console.warn('AudioWorklet not supported, falling back to ScriptProcessor:', err);
                }
            }

            // Resume context if it's suspended
            if (audioContext.state === 'suspended') {
                await audioContext.resume();
            }
        }

        async function initializeAudioStream() {
            try {
                const stream = await navigator.mediaDevices.getUserMedia({
                    audio: {
                        sampleRate: SAMPLE_RATE,
                        channelCount: NUM_CHANNELS,
                        autoGainControl: true,
                        echoCancellation: true,
                        noiseSuppression: true,
                        latency: 0.003
                    }
                });

                microphoneStream = stream;
                source = audioContext.createMediaStreamSource(stream);

                if (audioContext.audioWorklet) {
                    await setupAudioWorklet();
                } else {
                    setupScriptProcessor();
                }

                startAudioStats();
            } catch (error) {
                console.error('Error accessing microphone:', error);
                progressText.textContent = 'Error accessing microphone';
                stopAudio(true);
            }
        }

        async function setupAudioWorklet() {
            if (workletNode) {
                workletNode.disconnect();
            }

            workletNode = new AudioWorkletNode(audioContext, 'audio-processor', {
                numberOfInputs: 1,
                numberOfOutputs: 1,
                processorOptions: {
                    bufferSize: BUFFER_SIZE
                }
            });

            source.connect(workletNode);
            workletNode.connect(audioContext.destination);
            
            workletNode.port.onmessage = (event) => {
                if (ws && ws.readyState === WebSocket.OPEN) {
                    sendAudioData(event.data);
                }
            };
        }

        // ... rest of the previous code remains the same ...

        // Update start button handler
        async function startAudioBtnHandler() {
            if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
                alert('Audio input is not supported in your browser.');
                return;
            }

            try {
                await initializeAudio();
                startBtn.style.display = 'none';
                stopBtn.style.display = 'inline-block';
                stopBtn.disabled = false;
                isPlaying = true;

                initWebSocket();
            } catch (error) {
                console.error('Error initializing audio:', error);
                progressText.textContent = 'Error initializing audio system';
            }
        }

        // Update stop audio function
        function stopAudio(closeWebsocket) {
            isPlaying = false;
            playTime = 0;
            audioQueue = [];

            if (closeWebsocket && ws) {
                ws.close();
                ws = null;
            }

            if (workletNode) {
                workletNode.disconnect();
                workletNode = null;
            }

            if (scriptProcessor) {
                scriptProcessor.disconnect();
                scriptProcessor = null;
            }

            if (source) {
                source.disconnect();
                source = null;
            }

            if (microphoneStream) {
                microphoneStream.getTracks().forEach(track => track.stop());
                microphoneStream = null;
            }

            if (statsInterval) {
                clearInterval(statsInterval);
                statsInterval = null;
            }

            startBtn.style.display = 'inline-block';
            stopBtn.style.display = 'none';
            audioStats.textContent = '';
        }

        // Event listeners
        startBtn.addEventListener('click', startAudioBtnHandler);
        stopBtn.addEventListener('click', stopAudioBtnHandler);
        startBtn.disabled = true;
        stopBtn.disabled = true;
    </script>
</body>
</html>